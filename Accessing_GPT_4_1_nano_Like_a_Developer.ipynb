{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQt-gyAYUbm3"
   },
   "source": [
    "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PInACkIWUhOd"
   },
   "source": [
    "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecnJouXnUgKv",
    "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1pOrbwSU5H_"
   },
   "source": [
    "### Our First Prompt\n",
    "\n",
    "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
    "\n",
    "Let's create a `ChatCompletion` model to kick things off!\n",
    "\n",
    "There are three \"roles\" available to use:\n",
    "\n",
    "- `developer`\n",
    "- `assistant`\n",
    "- `user`\n",
    "\n",
    "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
    "\n",
    "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
    "\n",
    "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iy_LEPNEMVvC"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofMwuUQOU4sf",
    "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BuTv5aFsoKdmSUxlpZn53jxSpcNzu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks that facilitate building language model applications, particularly around document retrieval, processing, and conversational interfaces. However, they have different focuses, architectures, and use cases. Here's a comparison to clarify their differences:\\n\\n### LangChain\\n- **Primary Focus:** Building end-to-end language model applications with chaining, prompting, memory, and structured workflows.\\n- **Core Features:**\\n  - **Chains:** Modular sequences of calls, such as prompts, models, or tools.\\n  - **Agents:** Dynamic decision-making components that choose actions based on inputs.\\n  - **Memory:** Persistent context management for conversational or stateful interactions.\\n  - **Integrations:** Supports various LLM providers, APIs, tools, and document sources.\\n  - **Use Cases:** Chatbots, question-answering systems, automated workflows, agent-based applications.\\n- **Design Philosophy:** Emphasizes composability and flexibility to create complex language model applications.\\n\\n### LlamaIndex (GPT Index)\\n- **Primary Focus:** Efficiently indexing and querying large collections of documents or data sources for retrieval augmented generation (RAG) scenarios.\\n- **Core Features:**\\n  - **Indexing Data:** Converts documents into searchable indices.\\n  - **Querying:** Allows fast retrieval of relevant information to augment language model prompts.\\n  - **Data Connectors:** Supports various data sources like files, APIs, or databases.\\n  - **Memory & Storage:** Stores and manages data for quick retrieval.\\n  - **Use Cases:** Building retrieval-augmented question-answering systems, knowledge bases, document search engines.\\n- **Design Philosophy:** Simplifies the process of creating retrieval systems that work seamlessly with LLMs by managing indexing and retrieval.\\n\\n---\\n\\n### In summary:\\n| Aspect                         | **LangChain**                                           | **LlamaIndex (GPT Index)**                                 |\\n|--------------------------------|---------------------------------------------------------|------------------------------------------------------------|\\n| **Main Purpose**               | Building complex LLM applications and workflows        | Efficient document indexing and retrieval for RAG        |\\n| **Focus Area**                 | Application logic, chaining, agents, memory            | Data indexing, retrieval, and augmentation with LLMs     |\\n| **Use Cases**                  | Chatbots, automation, multi-step workflows            | Search engines, knowledge bases, retrieval-based QA     |\\n| **Flexibility**                | High; designed for custom, complex pipelines           | Focused on data ingestion, indexing, and retrieval       |\\n\\n---\\n\\n### Conclusion:\\n- If you're building complex applications involving multi-step processes, memory, or custom prompts, **LangChain** is ideal.\\n- If you're working on building retrieval systems or document-based question answering with large datasets, **LlamaIndex** offers powerful tools for indexing and fast retrieval.\\n\\nBoth tools can complement each other — for example, using LlamaIndex for document retrieval within a LangChain-powered application.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752801079, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=584, prompt_tokens=19, total_tokens=603, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX-7MnFhVNoT"
   },
   "source": [
    "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
    "\n",
    "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB76LJrDVgbc"
   },
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-vmtUV7WVOLW"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"developer\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osXgB_5nVky_"
   },
   "source": [
    "### Testing Helper Functions\n",
    "\n",
    "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
    "\n",
    "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "4yRwAWvgWFNq",
    "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Great question! Both **LangChain** and **LlamaIndex** (formerly known as GPT Index) are popular Python frameworks designed to facilitate building applications that leverage large language models (LLMs), but they serve different purposes and have distinct features. Here's a breakdown of their main differences:\n",
       "\n",
       "### Purpose and Use Cases\n",
       "\n",
       "- **LangChain:**\n",
       "  - Focuses on building **conversational agents, chatbots, and complex language model applications**.\n",
       "  - Emphasizes **chains, agents, and workflows**, allowing you to combine multiple components (e.g., prompts, LLM calls, memory, tools) to create sophisticated NLP pipelines.\n",
       "  - Supports integrations with various models, APIs, and tools for creating **interactive, dynamic applications**.\n",
       "\n",
       "- **LlamaIndex (GPT Index):**\n",
       "  - Primarily designed for **building indexes over large external data sources** (e.g., PDFs, documents, databases) so that LLMs can efficiently **retrieve and answer questions based on specific data**.\n",
       "  - Focuses on **document retrieval**, **semantic search**, and **question-answering over custom data collections**.\n",
       "  - Helps **ingest and query large custom datasets** with minimal setup, enabling LLMs to act as intelligent data assistants.\n",
       "\n",
       "### Core Functionality\n",
       "\n",
       "| Aspect | LangChain | LlamaIndex (GPT Index) |\n",
       "|---------|--------------|------------------------|\n",
       "| **Main Focus** | Building NLP workflows, chatbots, and agent-based applications | Indexing and querying large document sets or data sources |\n",
       "| **Data Handling** | Primarily works with prompts, memory, tools, and APIs; supports external data but less focused on indexing | Designed specifically for creating data indexes and retrieval mechanisms |\n",
       "| **Chain/Workflow Support** | Rich support for chains, agents, memory, toolkits to create complex interactions | Limited; more focused on data ingestion and retrieval |\n",
       "| **Integration with LLMs** | Extensive, supports multiple providers (OpenAI, Cohere, Hugging Face, etc.) | Focused on enabling LLMs to access and retrieve information from indexed data |\n",
       "\n",
       "### Use Case Examples\n",
       "\n",
       "- **LangChain:**\n",
       "  - Creating a conversational assistant that uses multiple tools and APIs.\n",
       "  - Building multi-step reasoning pipelines.\n",
       "  - Implementing agents that decide which tools to invoke during a conversation.\n",
       "\n",
       "- **LlamaIndex:**\n",
       "  - Building a search engine over internal company documents.\n",
       "  - Answering questions based on a custom dataset without training a new model.\n",
       "  - Indexing large documents so that LLMs can retrieve relevant excerpts during a Q&A session.\n",
       "\n",
       "### Summary\n",
       "\n",
       "| Feature                  | LangChain                                            | LlamaIndex                                   |\n",
       "|--------------------------|--------------------------------------------------------|----------------------------------------------|\n",
       "| Main purpose             | Complex NLP workflows, chatbots, multi-step chains   | Indexing and querying large data sources  |\n",
       "| Data ingestion           | Supports external data but not the primary focus      | Core focus: ingesting and indexing data    |\n",
       "| Use case specialization    | Workflow orchestration, agent-based systems          | Data retrieval, semantic search            |\n",
       "| Flexibility and extensibility | Highly flexible with chains and tools             | Specialized for document-based retrieval  |\n",
       "\n",
       "---\n",
       "\n",
       "**In brief:**  \n",
       "- Use **LangChain** if you're building conversational AI, agents, or complex NLP pipelines.  \n",
       "- Use **LlamaIndex** if you need to create a system where LLMs can efficiently access and answer questions based on large external datasets.\n",
       "\n",
       "Let me know if you'd like more detailed comparisons or examples!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPs3ScS1WpoC"
   },
   "source": [
    "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
    "\n",
    "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
    "\n",
    ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "aSX2F3bDWYgy",
    "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Are you kidding me? I don't have time to mess around—I am absolutely starving and just want some ice that actually satisfies! Crushed ice, while convenient, melts too fast and is a mess. Cubed ice is better because it lasts longer and keeps my drink colder without turning to water instantly. Honestly, I’m just desperate for something to eat, not some ice debate!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFs56KVaXuEY"
   },
   "source": [
    "Let's try that same prompt again, but modify only our system prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "CGOlxfcFXxJ7",
    "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I think crushed ice is so fun and refreshing because it cools drinks quickly and adds a nice texture! But cubed ice is perfect for keeping drinks colder longer without watering them down. Both have their charm—depends on what mood I’m in! How about you—do you prefer crushed or cubed ice?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkmjJd8zYQUK"
   },
   "source": [
    "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6b6z3CkYX9Y",
    "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BUc3g9V3hoAA0KyvjZI4YasY1mYOW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice is so fun and refreshing because it cools drinks quickly and adds a nice texture! But cubed ice is perfect for keeping drinks colder longer without watering them down. Both have their charm—depends on what mood I’m in! How about you—do you prefer crushed or cubed ice?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746635836, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_8fd43718b3', usage=CompletionUsage(completion_tokens=64, prompt_tokens=30, total_tokens=94, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqMRJLbOYcwq"
   },
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
    "\n",
    "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
    "\n",
    "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "iLfNEH8Fcs6c",
    "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here's a sentence using the words 'stimple' and 'falbean':\n",
       "\n",
       "\"During the peculiar festival, villagers gathered around a stimple, while children giggled over the mysterious falbean tucked into their baskets.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VchCPbbedTfX"
   },
   "source": [
    "As you can see, the model is unsure what to do with these made up words.\n",
    "\n",
    "Let's see if we can use the `assistant` role to show the model what these words mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "4InUN_ArZJpa",
    "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Here's a sentence using both \"stimple\" and \"falbean\":\n",
       "\n",
       "\"The stimple falbean crafted by the craftsmen ensures smooth rotation and reliable fastening for all our machinery.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0zn9-X2d23Z"
   },
   "source": [
    "As you can see, leveraging the `assistant` role makes for a stimple experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWUvXSWpeCs6"
   },
   "source": [
    "### Chain of Thought\n",
    "\n",
    "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions - but it can still benefit from a Chain of Thought Prompt to increase the reliability of the response!\n",
    "\n",
    "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "cwW0IgbfeTwP",
    "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze the options carefully:\n",
       "\n",
       "**Option 1:** Fly (3 hours) + Bus (2 hours)  \n",
       "Total travel time: 3 + 2 = 5 hours\n",
       "\n",
       "**Option 2:** Teleporter (0 hours) + Bus (1 hour)  \n",
       "Total travel time: 0 + 1 = 1 hour\n",
       "\n",
       "**Current local time:** 1PM\n",
       "\n",
       "**Target arrival time:** before 7PM EDT\n",
       "\n",
       "Since the current local time is 1PM and Billy wants to arrive home before 7PM EDT (which is 6 hours later), he has a window of nearly 6 hours to get home.\n",
       "\n",
       "**Calculating arrival times:**\n",
       "\n",
       "- **Option 1:**  \n",
       "  Departure at 1PM local time, travel takes 5 hours, arriving around 6PM local time.  \n",
       "  Since this is within the 6-hour window, Billy would arrive just before 7PM EDT.\n",
       "\n",
       "- **Option 2:**  \n",
       "  Departure at 1PM, travel takes 1 hour, arriving around 2PM local time, well before 7PM EDT.\n",
       "\n",
       "**Conclusion:**  \n",
       "Yes, it does matter which option Billy chooses if he needs to arrive strictly before 7PM EDT. The teleportation + bus option ensures he arrives much earlier, giving him more buffer time. The flying + bus option just makes it in time, arriving right around 6PM local time, which is still before 7PM EDT.\n",
       "\n",
       "**Final note:**  \n",
       "- If Billy prefers certainty and plenty of extra time, the teleport + bus is better.  \n",
       "- If he wants to save time and is okay arriving close to 7PM, the flying + bus is sufficient.\n",
       "\n",
       "**Answer:** Yes, the choice matters if arriving strictly before 7PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same prompt with a small modification - but this time include \"Let's think step by step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze the options step by step:\n",
       "\n",
       "**Current situation:**\n",
       "- It is currently 1PM local time.\n",
       "- Billy wants to arrive home **before 7PM EDT**.\n",
       "\n",
       "**Important considerations:**\n",
       "- Time zones are not explicitly specified, but since Billy is in San Francisco (Pacific Time, PT), and the deadline is in EDT, we need to convert times accordingly.\n",
       "- Pacific Time (PT) is **3 hours behind Eastern Time (ET)**.\n",
       "  - When it's 1PM PT, it's **4PM ET**.\n",
       "\n",
       "**Conversion:**\n",
       "- **Current local time:** 1PM PT = 4PM ET\n",
       "- **Deadline:** 7PM ET\n",
       "\n",
       "Billy needs to arrive **before 7PM ET**, which is **before 7PM ET**.\n",
       "\n",
       "---\n",
       "\n",
       "### Option 1: Fly + Bus\n",
       "- Flying takes **3 hours**.\n",
       "- Bus takes **2 hours**.\n",
       "\n",
       "**Total travel time:** 3 + 2 = **5 hours**\n",
       "\n",
       "### Option 2: Teleporter + Bus\n",
       "- Teleporter takes **0 hours**.\n",
       "- Bus takes **1 hour**.\n",
       "\n",
       "**Total travel time:** 0 + 1 = **1 hour**\n",
       "\n",
       "---\n",
       "\n",
       "### Now, let's calculate the arrival times for each option:\n",
       "\n",
       "---\n",
       "\n",
       "### Option 1: Fly + Bus\n",
       "\n",
       "- Departure time: 1PM PT (which is 4PM ET)\n",
       "- Travel duration: 5 hours\n",
       "- Arrival time in ET: 4PM + 5 hours = **9PM ET**\n",
       "\n",
       "**Note:** Since he departs at 1PM PT (=4PM ET), and takes 5 hours, he'd arrive **at 9PM ET**.\n",
       "\n",
       "**Conclusion:** He arrives **after 7PM ET**. **Not** before the deadline.\n",
       "\n",
       "---\n",
       "\n",
       "### Option 2: Teleporter + Bus\n",
       "\n",
       "- Departure time: 1PM PT (=4PM ET)\n",
       "- Travel duration: 1 hour\n",
       "- Arrival time in ET: 4PM + 1 hour = **5PM ET**\n",
       "\n",
       "**Conclusion:** He arrives **before 7PM ET**.\n",
       "\n",
       "---\n",
       "\n",
       "### Final answer:\n",
       "**Yes, it does matter which option Billy chooses.** \n",
       "\n",
       "- The teleporter + bus allows him to arrive **before the deadline**.\n",
       "- The fly + bus option makes him arrive **after the deadline**.\n",
       "\n",
       "**Therefore, Billy should choose the teleporter + bus option to reach home before 7PM EDT.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFcrU-4pgRBS"
   },
   "source": [
    "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
    "\n",
    "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k9TKR1DhWI2"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
    "\n",
    "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/The-AI-Engineer-Challenge) repository!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rGI1nJeqeO_"
   },
   "source": [
    "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
